{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb85658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:45:00.092039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-13 09:45:00.092066: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import serial\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "from serial.tools.list_ports import comports\n",
    "\n",
    "def read_number(msg):\n",
    "    return int(input(msg))\n",
    "\n",
    "def read_port(msg):\n",
    "    port = input(msg)\n",
    "    #index = input(msg)\n",
    "    #port = \"COM8\";\n",
    "    return serial.Serial(port, 9600)\n",
    "\n",
    "            \n",
    "def print_until_keyword(keyword, device):\n",
    "    while True: \n",
    "        msg = device.serial.readline().decode()\n",
    "        if msg[:-2] == keyword: break\n",
    "        #else: print(f'({arduino.port}):',msg, end='')\n",
    "            \n",
    "def read_matrix(device, dimms):\n",
    "    result = np.zeros((1,dimms)).reshape(-1)\n",
    "    for i in range(dimms):\n",
    "        device.serial.read()\n",
    "        result[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "    \n",
    "    return result.reshape(dimms)\n",
    "\n",
    "clear = lambda: os.system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device:\n",
    "    def __init__(self, serial):\n",
    "        self.serial = serial\n",
    "        self.weights = []\n",
    "        self.metalayers = []\n",
    "        self.gradients = []\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weight\n",
    "        \n",
    "    def set_metadata(self, metalayer):\n",
    "        self.metalayer = metalayer\n",
    "        \n",
    "def getDevices():\n",
    "    num_devices = read_number(\"Number of devices: \")\n",
    "    # num_devices = 2\n",
    "    #num_devices=3\n",
    "\n",
    "    available_ports = comports()\n",
    "    print(\"Available ports:\")\n",
    "    for available_port in available_ports: print(available_port)\n",
    "\n",
    "    time.sleep(1)\n",
    "    devices = [read_port(f\"Port device_{i+1}: \") for i in range(num_devices)]\n",
    "    #devices= [serial.Serial(port, 9600) for port in ['/dev/cu.usbmodem14101', '/dev/cu.usbmodem14201','/dev/cu.usbmodem14301']]\n",
    "    list_devices = []\n",
    "    for device in devices:\n",
    "        list_devices.append(Device(device))\n",
    "    return list_devices\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_type):\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_type}\"\n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, rows, cols):\n",
    "        super().__init__(\"Dense\")\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_type=} {self.rows=} {self.cols=}\"\n",
    "    \n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, r, c, ch):\n",
    "        super().__init__(\"MaxPooling\")\n",
    "        self.rows = r\n",
    "        self.cols = c\n",
    "        self.ch = ch\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_type}  - {self.rows=} {self.cols=} {self.ch=}\"\n",
    "    \n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, kh, kw, c, kc):\n",
    "        super().__init__(\"Conv2D\")\n",
    "        self.kh = kh\n",
    "        self.kw = kw\n",
    "        self.c = c\n",
    "        self.kc = kc\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_type=} - {self.kh=} {self.kw=} {self.c=} {self.kc=}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa60eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_model_info(device):\n",
    "    device.serial.reset_input_buffer()\n",
    "    device.serial.write(b's')\n",
    "    print_until_keyword('start', device) # CLEAN SERIAL\n",
    "    \n",
    "    bytesToRead = device.serial.read(1).decode()\n",
    "    time.sleep(1)\n",
    "    if bytesToRead == 'i':\n",
    "        device.serial.write(struct.pack('f', LEARNING_RATE))\n",
    "        [num_layers] = struct.unpack('i', device.serial.read(4))\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            [layer_type] = struct.unpack('i', device.serial.read(4))\n",
    "            if layer_type == -1:\n",
    "                [rows, cols] = struct.unpack('ii', device.serial.read(8))\n",
    "                layers.append(Dense(rows, cols))\n",
    "            elif layer_type == -2:\n",
    "                [rows, cols, ch] = struct.unpack('iii', device.serial.read(12))\n",
    "                layers.append(MaxPooling(rows,cols,ch))\n",
    "            elif layer_type == -3:\n",
    "                [kh, kw, c, kc] = struct.unpack('iiii', device.serial.read(16))\n",
    "                layers.append(Conv2D(kh,kw,c,kc))\n",
    "            # dimms.append((1,cols)) #Â bias\n",
    "            # dimms.append((rows,cols)) # matrix weigths\n",
    "        device.metalayers = layers\n",
    "    return num_layers, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d79225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random    \n",
    "## RECEIVE MODEL WEIGHT\n",
    "def get_device_weights(device, bias_dimm, w_dimm):\n",
    "    number_of_floats = w_dimm[0] * w_dimm[1]\n",
    "    weights = np.zeros(w_dimm).reshape(-1)\n",
    "    for i in range(number_of_floats):\n",
    "        device.serial.read()\n",
    "        weights[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "        \n",
    "    number_of_floats = bias_dimm[0] * bias_dimm[1]\n",
    "    bias = np.zeros(bias_dimm).reshape(-1)\n",
    "    for i in range(number_of_floats):\n",
    "        device.serial.read()\n",
    "        bias[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "    \n",
    "    return weights.reshape(w_dimm), bias.reshape(bias_dimm)\n",
    "    \n",
    "def get_device_weights_cnn(device, kh, kw, c, kc):\n",
    "    in_size = kh * kw * c\n",
    "    out_size = kh * kw * kc\n",
    "    weights = np.zeros((kh, kw, c, kc)).reshape(-1)\n",
    "    for i,w in enumerate(weights.reshape(-1)):\n",
    "        device.serial.read()\n",
    "        weights[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "            \n",
    "    bias = np.zeros((1,kc)).reshape(-1)\n",
    "    for i,b in enumerate(bias.reshape(-1)):\n",
    "        device.serial.read()\n",
    "        bias[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "    \n",
    "    return weights.reshape((kh, kw, c, kc)), bias.reshape((1,kc))\n",
    "\n",
    "def get_device_weights_pool(device, rows, cols, ch):\n",
    "    weights = np.random.randn(rows, cols, ch).reshape(-1)\n",
    "    for i,w in enumerate(weights.reshape(-1)):\n",
    "        device.serial.read()\n",
    "        weights[i] = struct.unpack('f', device.serial.read(4))[0]\n",
    "    \n",
    "    return weights.reshape((rows, cols, ch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ef76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_device_weights(device, bias_dimm, w_dimm):\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    bias = np.zeros(bias_dimm)\n",
    "    weights = np.random.randn(w_dimm[0], w_dimm[1]) * np.sqrt(6.0 / (w_dimm[0] + w_dimm[1]))\n",
    "    return weights, bias\n",
    "    device.serial.write(struct.pack('f'*bias.reshape(-1).shape[0], *bias.reshape(-1)))\n",
    "\n",
    "    #for w in weights.reshape(-1):\n",
    "    #   data = device.serial.read()\n",
    "    device.serial.write(struct.pack('f'*weights.reshape(-1).shape[0], *weights.reshape(-1)))\n",
    "    \n",
    "def initialize_device_weights_cnn(device, kh, kw, c, kc):\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    bias = np.zeros((1,kc))\n",
    "    in_size = kh * kw * c\n",
    "    out_size = kh * kw * kc\n",
    "    weights = np.random.randn(kh, kw, c, kc) * np.sqrt(6.0 / (in_size + out_size))\n",
    "    return weights, bias\n",
    "    print(f\"Sending weights for layer Conv2D\")\n",
    "    #for b in bias.reshape(-1):\n",
    "        #data = device.serial.read()\n",
    "    device.serial.write(struct.pack('f'* bias.reshape(-1).shape[0], *bias.reshape(-1)))\n",
    "\n",
    "    #for w in weights.reshape(-1):\n",
    "    #   data = device.serial.read()\n",
    "    device.serial.write(struct.pack('f' * weights.reshape(-1).shape[0], *weights.reshape(-1)))\n",
    "\n",
    "def create_initial_model(device, layers):\n",
    "    weights = []\n",
    "    for layer in layers:\n",
    "        if layer.layer_type == \"Conv2D\":\n",
    "            weights.append(initialize_device_weights_cnn(device, layer.kh, layer.kw, layer.c, layer.kc))\n",
    "        elif layer.layer_type == \"Dense\":\n",
    "            weights.append(initialize_device_weights(device, (1,layer.cols), (layer.rows,layer.cols)))\n",
    "        else:\n",
    "            weights.append((np.array([]), np.array([])))\n",
    "    return weights\n",
    "\n",
    "    \n",
    "def send_initial_weights(device, global_model):\n",
    "    for weights, bias in global_model:\n",
    "        #print(weights.shape, bias.shape)\n",
    "        device.serial.write(struct.pack('f' * bias.reshape(-1).shape[0], *bias.reshape(-1)))\n",
    "        device.serial.write(struct.pack('f' * weights.reshape(-1).shape[0], *weights.reshape(-1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb19c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_weights(device, weights, layer):\n",
    "    bias = weights[1]\n",
    "    weights = weights[0]\n",
    "    #print(f\"Sending bias for Dense {bias.reshape(-1).shape} {device.serial.port}\")\n",
    "    #for b in bias.reshape(-1):\n",
    "        #data = device.serial.read()\n",
    "        #device.serial.write(struct.pack('f', b))\n",
    "    if layer.layer_type != 'MaxPooling':\n",
    "        device.serial.write(struct.pack('f' * bias.reshape(-1).shape[0],*bias.reshape(-1)))\n",
    "        device.serial.write(struct.pack('f' * weights.reshape(-1).shape[0], *weights.reshape(-1)))\n",
    "\n",
    "    #print(f\"Sending weights for Dense {weights.reshape(-1).shape} {device.serial.port}\")\n",
    "    #for w in weights.reshape(-1):\n",
    "        #data = device.serial.read()\n",
    "        \n",
    "def send_model_weights(device, weights):\n",
    "    layers = device.metalayers\n",
    "    device.serial.write(b'r')\n",
    " \n",
    "    for i, layer in enumerate(weights):\n",
    "        send_weights(device, weights[i], layers[i])\n",
    "\n",
    "    #print(f\"{device.serial.port} weights initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e51a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(device):\n",
    "    layers = device.metalayers\n",
    "    device.serial.write(b'g') # Python --> ACK --> Arduino\n",
    "    device.weights = []\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        if layer.layer_type == \"Conv2D\":\n",
    "            weights, biases = get_device_weights_cnn(device, layer.kh, layer.kw, layer.c, layer.kc)\n",
    "            device.weights.append((weights, biases))\n",
    "        elif layer.layer_type == \"Dense\":\n",
    "            weights, biases = get_device_weights(device, (1,layer.cols), (layer.rows,layer.cols))\n",
    "            device.weights.append((weights, biases))\n",
    "        elif layer.layer_type == \"MaxPooling\":\n",
    "            #weights = get_device_weights_pool(device, layer.rows,layer.cols, layer.ch)\n",
    "            device.weights.append((weights, np.array([])))\n",
    "    #print(f\"Model weight received!\")\n",
    "\n",
    "    \n",
    "def get_model_gradients(device):\n",
    "    layers = device.metalayers\n",
    "    device.serial.write(b'g') # Python --> ACK --> Arduino\n",
    "    device.gradients = []\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if layer.layer_type == \"Conv2D\":\n",
    "            weights, biases = get_device_weights_cnn(device, layer.kh, layer.kw, layer.c, layer.kc)\n",
    "            device.gradients.append((weights, biases))\n",
    "        elif layer.layer_type == \"Dense\":\n",
    "            weights, biases = get_device_weights(device, (1,layer.cols), (layer.rows,layer.cols))\n",
    "            device.gradients.append((weights, biases))\n",
    "        elif layer.layer_type == \"MaxPooling\":\n",
    "            #weights = get_device_weights_pool(device, layer.rows,layer.cols, layer.ch)\n",
    "            device.gradients.append((np.array([]), np.array([])))\n",
    "    #print(f\"Model weight received!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab0651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_sample(device, X, y=None):\n",
    "    #if IS_KEYWORD_SPOTTING:\n",
    "        #for i,k in enumerate(X.reshape(-1)):\n",
    "            #device.serial.write(struct.pack('f',k))\n",
    "            #print(\"SENT XX\",i)\n",
    "        #device.serial.write(struct.pack('h' * X.reshape(-1).shape[0], *X.reshape(-1)))\n",
    "    #else:\n",
    "    device.serial.write(struct.pack('f' * X.reshape(-1).shape[0], *X.reshape(-1)))\n",
    "\n",
    "        #for i,k in enumerate(X.reshape(-1)):\n",
    "            #raw = device.serial.read(4)\n",
    "            #j = struct.unpack('i', raw)[0]\n",
    "            #device.serial.write(struct.pack('f',k))\n",
    "            #print(\"SENT\",j, i)\n",
    "            #print(f\"returned error = {raw}\")\n",
    "\n",
    "    #print(f\"Want to send y={y.shape}\")\n",
    "    if y is not None:\n",
    "        device.serial.write(struct.pack('f' * y.reshape(-1).shape[0], *y.reshape(-1)))\n",
    "        #for i, t in enumerate(y.reshape(-1)):\n",
    "            #device.serial.read()\n",
    "            #device.serial.write(struct.pack('f', t))\n",
    "        #print(f\"Sample y = {y.shape} sent!\")\n",
    "\n",
    "def get_tick():\n",
    "    return time.time_ns()\n",
    "\n",
    "def train_single(device, x, y, train_data):\n",
    "    device.serial.write(b\"t\")\n",
    "    send_sample(device, x, y.reshape(1,TARGET_SIZE))\n",
    "    start = get_tick()\n",
    "    output = read_matrix(device, TARGET_SIZE+1)\n",
    "    end = get_tick()\n",
    "    train_data['losses'][device.serial.port] = output[-1]\n",
    "    train_data['times'][device.serial.port] = end-start\n",
    "    res = output[:-1]\n",
    "    train_data['acc'][device.serial.port] = 1 if np.argmax(y) == np.argmax(res) else 0\n",
    "\n",
    "\n",
    "def train(device, X, y, size=1):\n",
    "    error = 0.0\n",
    "    for i in range(size):\n",
    "        if i%100 == 0:\n",
    "            print(f\"{i}/{size} Done\")\n",
    "        device.serial.write(b\"t\")\n",
    "        send_sample(device, X[i], y[i].reshape(1,TARGET_SIZE))\n",
    "        #print(\"SAMPLE SENT\")\n",
    "\n",
    "        start = get_tick()\n",
    "        output = read_matrix(device, TARGET_SIZE+1)\n",
    "        #n_error = device.serial.read(4)\n",
    "        #print(f\"returned error = {n_error}\")\n",
    "        end = get_tick()\n",
    "        loss = struct.unpack('f', n_error)[0]\n",
    "        error += loss\n",
    "    return end-start, error/size\n",
    "\n",
    "def predict(device, X, y):\n",
    "    device.serial.write(b\"p\")\n",
    "    send_sample(device, X, y.reshape(1,TARGET_SIZE))\n",
    "    # read last layer size output\n",
    "    # Target size + 1 for the loss value\n",
    "    start = get_tick()\n",
    "    output = read_matrix(device, TARGET_SIZE+1)\n",
    "    return get_tick() - start, output[:-1], output[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce25415",
   "metadata": {},
   "source": [
    "- call getDevices() to obtain all conected devices\n",
    "- asks the user how many devices you want to use\n",
    "- send the initial model for every device\n",
    "- create thread for every device\n",
    "    - send samples and start training for one epoch\n",
    "- wait for all threads to finish\n",
    "- FEDERATED LEARNING\n",
    "- create thread for every device and receive models\n",
    "- permute the average of every layer\n",
    "- create thread for every device and receive models\n",
    "- send back the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684bdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = []\n",
    "def fl(devices):\n",
    "    global final_weights\n",
    "    \n",
    "    # RECEIVE MODELS\n",
    "    #print(\"Receiving models from devices...\")\n",
    "    threads = []\n",
    "    for device in devices:\n",
    "        thread = threading.Thread(target=get_model_weights, args=(device,))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads: thread.join() # Wait for all the threads to end\n",
    "    #print(\"Models received\")\n",
    "    # AVERAGE MODELS\n",
    "    num_layers = len(devices[0].metalayers)\n",
    "    assert num_layers > 0, \"NO LAYERS!\"\n",
    "\n",
    "    #accuracies = fl_info_infer[-2]['accuracy']\n",
    "    #total_accuracy = sum(accuracies.values())\n",
    "    #normalized_accuracies = [accuracy / total_accuracy for accuracy in accuracies.values()]\n",
    "    #print(normalized_accuracies)\n",
    "    list_weights = []\n",
    "    for i in range(num_layers):\n",
    "        weights = np.array([device.weights[i][0] for device in devices])\n",
    "        bias = np.array([device.weights[i][1] for device in devices])\n",
    "\n",
    "        weights_avg = np.mean(weights, axis=0)\n",
    "        bias_avg = np.mean(bias, axis=0)\n",
    "\n",
    "        list_weights.append((weights_avg, bias_avg))\n",
    "    #print(\"Average performed\")\n",
    "    # send model\n",
    "    threads = []\n",
    "    for device in devices:\n",
    "        thread = threading.Thread(target=send_model_weights, args=(device, list_weights))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads: thread.join() # Wait for all the threads to end\n",
    "        \n",
    "    final_weights = list_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08114bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_sgd(devices):\n",
    "    # RECEIVE MODELS\n",
    "    threads = []\n",
    "    for device in devices:\n",
    "        thread = threading.Thread(target=get_model_gradients, args=(device,))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads: thread.join() # Wait for all the threads to end\n",
    "    #print(\"Models received\")\n",
    "    # AVERAGE MODELS\n",
    "    num_layers = len(devices[0].metalayers)\n",
    "    assert num_layers > 0, \"NO LAYERS!\"\n",
    "    \n",
    "    # GET GRADIENTS OF THE DEVICES\n",
    "    list_weights = []\n",
    "    for i in range(num_layers):\n",
    "        gradients_w, gradients_b = devices[0].gradients[i]\n",
    "        gradients_w = np.zeros_like(gradients_w)\n",
    "        gradients_b = np.zeros_like(gradients_b)\n",
    "        for k,device in enumerate(devices):\n",
    "            #print(device.gradients[i][0])\n",
    "            #print(device.gradients[i][1])\n",
    "            gradients_w += device.gradients[i][0]\n",
    "            gradients_b += device.gradients[i][1]\n",
    "        \n",
    "        list_weights.append((gradients_w,gradients_b))\n",
    "\n",
    "    # SGD HERE\n",
    "    for i, layer in enumerate(devices[0].metalayers):\n",
    "        print(\"FF\",list_weights[i][0], list_weights[i][1])\n",
    "        if layer.layer_type == \"MaxPooling\":\n",
    "            continue\n",
    "        dw = iglobal_model[i][0] - list_weights[i][0] * 0.01\n",
    "        db = iglobal_model[i][1] - list_weights[i][1] * 0.01\n",
    "        \n",
    "        iglobal_model[i] = (dw, db)\n",
    "    \n",
    "    \n",
    "    # send model\n",
    "    threads = []\n",
    "    for device in devices:\n",
    "        thread = threading.Thread(target=send_model_weights, args=(device, iglobal_model))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads: thread.join() # Wait for all the threads to end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bacffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_device(device, X_train, Y_train,  info, steps = 1):\n",
    "    device_losses = []\n",
    "    device_times = []\n",
    "    if not isinstance(X_train, np.ndarray):\n",
    "        X_train = np.array(X_train)\n",
    "    \n",
    "    if not isinstance(Y_train, np.ndarray):\n",
    "        Y_train = np.array(Y_train)\n",
    "    \n",
    "    dt, loss = train(device, np.array(X_train).astype(np.int16), Y_train, steps)\n",
    "    device_losses.append(loss)\n",
    "    device_times.append(dt)\n",
    "        \n",
    "    info['losses'][device.serial.port] = device_losses\n",
    "    info['train_time'][device.serial.port] = device_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_device(device, X, Y_test, info):\n",
    "    device_times = []\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    df_time = 0\n",
    "    for i, x in enumerate(X):\n",
    "        if i%100 == 0:\n",
    "            print(f\"{device.serial.port} => [{i}/{X.shape[0]}] Done\")\n",
    "        \n",
    "        dt, res, loss = predict(device, x, Y_test[i])\n",
    "        acc += 1 if np.argmax(Y_test[i]) == np.argmax(res) else 0\n",
    "        df_time += dt\n",
    "        total_loss += loss\n",
    "    #info['accuracy'][device.serial.port] = acc/X.shape[0 #FF commented\n",
    "    #info['infer_time'][device.serial.port] = df_time / X.shape[0] #FF commented\n",
    "    info['losses'][device.serial.port] = total_loss/X.shape[0]\n",
    "    info['res'][device.serial.port] = res #FF uncommented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_anomaly():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    # Read the JSON file\n",
    "    with open(\"datasets_2/stateMonitors.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Normalize the JSON data\n",
    "    df = pd.json_normalize(data, max_level=20)\n",
    "    df['anomaly'] = 0\n",
    "\n",
    "    # Specify the size of the random subset\n",
    "    subset_size = 200  # Replace with the desired subset size\n",
    "\n",
    "    # Get a random subset of the DataFrame\n",
    "    random_subset = df.sample(n=subset_size, random_state=42)  # Replace '42' with your desired random seed\n",
    "\n",
    "    cols_to_rand = ['payload.messageSize','payload.state.QR','payload.state.QS','payload.state.QRU','payload.state.QWRP','payload.state.RT','payload.state.packetHeader.Size']\n",
    "    for col in cols_to_rand:\n",
    "        random_subset[col] += np.random.randint(2000)\n",
    "\n",
    "    random_subset['anomaly'] = 1\n",
    "    \n",
    "    return df, random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_device(device):\n",
    "    device.serial.write(b'f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b209c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "devices = []\n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_SIZE = 10\n",
    "IS_KEYWORD_SPOTTING = False\n",
    "def initialize_devices(lr = 0.001, target_size = 10, first=True):\n",
    "    global devices\n",
    "    global LEARNING_RATE\n",
    "    global TARGET_SIZE\n",
    "    \n",
    "    TARGET_SIZE = target_size\n",
    "    LEARNING_RATE = lr\n",
    "    if first:\n",
    "        devices = getDevices()\n",
    "    else:\n",
    "        threads = []\n",
    "        for device in devices:\n",
    "            print(f\"RESETING DEVICE {device.serial.port}\")\n",
    "            thread = threading.Thread(target=reset_device, args=(device, ))\n",
    "            thread.daemon = True\n",
    "            threads.append(thread)\n",
    "\n",
    "          # Start all the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for all the threads to finish\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "    number_devices = 3\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    threads = []\n",
    "    for i, d in enumerate(devices):\n",
    "        print(f\"Receiving model info {i}\")\n",
    "        thread = threading.Thread(target=receive_model_info, args=(d, ))\n",
    "        thread.daemon = True\n",
    "        threads.append(thread)\n",
    "\n",
    "      # Start all the threads\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all the threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    iglobal_model = create_initial_model(devices[0], devices[0].metalayers)\n",
    "\n",
    "    threads = []\n",
    "    for i, d in enumerate(devices):\n",
    "        print(f\"Sending blank model for device {i}\")\n",
    "        thread = threading.Thread(target=send_initial_weights, args=(d, iglobal_model ))\n",
    "        thread.daemon = True\n",
    "        threads.append(thread)\n",
    "\n",
    "      # Start all the threads\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all the threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    print(\"All devices were initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99a389",
   "metadata": {},
   "source": [
    "## Anomaly Detection FL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ea693",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "X, anomalies = get_anomaly()\n",
    "y = X['anomaly']\n",
    "df = X.drop(['date','anomaly'], axis=1)\n",
    "y_anomalies = anomalies['anomaly']\n",
    "anomalies = anomalies.drop(['date','anomaly'], axis=1)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply z-score normalization to the DataFrame\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "# Create a StandardScaler object\n",
    "scaler2 = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler2.fit(df)\n",
    "\n",
    "# Apply the same normalization to the test data\n",
    "df_test_normalized = pd.DataFrame(scaler2.transform(anomalies), columns=anomalies.columns)\n",
    "df_test_normalized.head()\n",
    "\n",
    "df_test_normalized = pd.concat([df_normalized, df_test_normalized])\n",
    "df_test_normalized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "def train_ae_with_fl(df, num_devices=3, fl_samples=5, lr=0.001, first=True, iid=True):\n",
    "    initialize_devices(lr=lr, target_size=25, first=first)\n",
    "    fl_rounds = 0\n",
    "    \n",
    "    X_Train = []\n",
    "    total_entries = 2305\n",
    "    each_split_size = total_entries // 3\n",
    "\n",
    "    X_Train.append(df_normalized.values[0:each_split_size])\n",
    "    X_Train.append(df_normalized.values[each_split_size:each_split_size*2])\n",
    "    X_Train.append(df_normalized.values[each_split_size*2:each_split_size*3])\n",
    "\n",
    "    training_data = []\n",
    "    total_acc = {'/dev/cu.usbmodem13201':0, '/dev/cu.usbmodem13101':0, '/dev/cu.usbmodem13301':0}\n",
    "    for i in range(each_split_size):\n",
    "        if i%fl_samples==0 and i>0:\n",
    "            fl(devices)\n",
    "            fl_rounds += 1\n",
    "        train_data = {'losses':{}, 'times':{}, 'acc':{}}\n",
    "        threads = []\n",
    "        for j, device in enumerate(devices):\n",
    "            thread = threading.Thread(target=train_single, args=(device, X_Train[j][i],X_Train[j][i], train_data))\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads: thread.join() # Wait for all the threads to end\n",
    "        training_data.append(train_data)\n",
    "        print(f\"Loss: {train_data['losses']} {i}/{each_split_size}\")\n",
    "\n",
    "    # SET THE DEVICES WEIGHT AS THE GLOBAL MODEL\n",
    "    fl(devices)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bd39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_dict = train_ae_with_fl(df,num_devices=3, fl_samples=25, lr=0.01, first=True, iid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = 3 #3\n",
    "fl_every_epoch = 4 # number of epochs to execute fl\n",
    "epochs = 1 # number of epochs to execute before every fl\n",
    "fl_info_train = []\n",
    "fl_info_infer = []\n",
    "# uncomment this line to use the entire dataset\n",
    "# steps = X_train.shape[0]\n",
    "# 227451\n",
    "#train_steps = X_train.shape[0]\n",
    "#test_steps = X_test.shape[0]\n",
    "# samples_each_device\n",
    "#sdev = train_steps // num_devices\n",
    "#tsdev = test_steps // num_devices\n",
    "#print(f\"Each device will receive {sdev} elements\")\n",
    "\n",
    "#X_train, Y_train, X_test, Y_test = get_fashion(iid=True)\n",
    "X_train, Y_train, X_test, Y_test = get_anomaly() #FF added\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #info_infer = {'accuracy':{}, 'infer_time':{}, 'losses':{}, 'res':{}}\n",
    "    info_infer = {'losses':{}, 'res':{}} # FF added\n",
    "    threads = []\n",
    "    #for device in devices:\n",
    "    predict_device(devices[1], X_test, Y_test, info_infer)\n",
    "    #for thread in threads: thread.join() # Wait for all the threads to end\n",
    "    fl_info_infer.append(info_infer)\n",
    "    #print(fl_info_infer)\n",
    "    #info = {'losses':{}, 'train_time':{}}\n",
    "    #threads = []\n",
    "    #for i,device in enumerate(devices):\n",
    "     #   thread = threading.Thread(target=train_device, args=(device, X_train[i*sdev:(i+1)*sdev], Y_train[i*sdev:(i+1)*sdev], info, sdev))\n",
    "     #   thread.daemon = True\n",
    "     #   thread.start()\n",
    "     #   threads.append(thread)\n",
    "    #for thread in threads: thread.join() # Wait for all the threads to end\n",
    "    #print(f\"Epochs => {epoch+1}/{epochs} done\")\n",
    "    #fl_info_train.append(info)\n",
    "    \n",
    "        #do federated learning\n",
    "    #if epoch%fl_every_epoch==0 and epoch > 0:\n",
    "     #   fl(devices)\n",
    "     #   print(\"FedSGD Done.\")\n",
    "        \n",
    "    #global_loss = 0.0\n",
    "    #for d in fl_info_train[-1]['losses']:\n",
    "    #    global_loss += fl_info_train[-1]['losses'][d][0]\n",
    "    #print(global_loss/3)\n",
    "\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a018ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "def train_ae_with_fl(df, num_devices=3, fl_samples=5, lr=0.001, first=True, iid=True):\n",
    "    initialize_devices(lr=lr, target_size=25, first=first)\n",
    "    fl_rounds = 0\n",
    "    \n",
    "    X_Train = []\n",
    "    total_entries = 2305\n",
    "    each_split_size = total_entries // 3\n",
    "\n",
    "    X_Train.append(df_normalized.values[0:each_split_size])\n",
    "    X_Train.append(df_normalized.values[each_split_size:each_split_size*2])\n",
    "    X_Train.append(df_normalized.values[each_split_size*2:each_split_size*3])\n",
    "\n",
    "    training_data = []\n",
    "    total_acc = {'/dev/cu.usbmodem13201':0, '/dev/cu.usbmodem13101':0, '/dev/cu.usbmodem13301':0}\n",
    "    for i in range(each_split_size):\n",
    "        if i%fl_samples==0 and i>0:\n",
    "            fl(devices)\n",
    "            fl_rounds += 1\n",
    "        train_data = {'losses':{}, 'times':{}, 'acc':{}}\n",
    "        threads = []\n",
    "        for j, device in enumerate(devices):\n",
    "            thread = threading.Thread(target=train_single, args=(device, X_Train[j][i],X_Train[j][i], train_data))\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads: thread.join() # Wait for all the threads to end\n",
    "        training_data.append(train_data)\n",
    "        # print(f\"Loss: {train_data['losses']} {i}/{each_split_size}\")\n",
    "\n",
    "    # SET THE DEVICES WEIGHT AS THE GLOBAL MODEL\n",
    "    fl(devices)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a88c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = train_ae_with_fl(df,num_devices=3, fl_samples=25, lr=0.01, first=True, iid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(get_info(loaded_dict, 'losses'), samples=768, step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59759599",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_losses = []\n",
    "for i,x in enumerate(df_test_normalized.values):\n",
    "    dt, res, loss = predict(devices[0],df_test_normalized.values[i], df_test_normalized.values[i])\n",
    "    x_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(x_losses)\n",
    "r.columns = ['loss']\n",
    "y_df = pd.DataFrame(y)\n",
    "y_anomalies_df = pd.DataFrame(y_anomalies)\n",
    "\n",
    "ys = pd.concat([y_df, y_anomalies_df])\n",
    "ys['loss'] = r['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set a threshold to classify data as normal or anomalous\n",
    "threshold = 1.3\n",
    "y_pred = [1 if error > threshold else 0 for error in r['loss']]\n",
    "# Compute evaluation metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming 'ys' is a DataFrame or Series containing the ground truth labels ('anomaly')\n",
    "# Assuming 'y_pred' is a list, DataFrame, or Series containing the predicted labels\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(ys['anomaly'], y_pred)\n",
    "precision = precision_score(ys['anomaly'], y_pred)\n",
    "recall = recall_score(ys['anomaly'], y_pred)\n",
    "f1 = f1_score(ys['anomaly'], y_pred)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(ys['anomaly'], y_pred, labels=[0, 1])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d72cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
